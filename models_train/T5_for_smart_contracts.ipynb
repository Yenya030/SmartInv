{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sallywang147/llm_invariants/blob/master/T5_for_smart_contracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbwl6E1E205R"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from numpy import random\n",
        "import pandas as pd\n",
        "import gspread\n",
        "import gc\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('invariants_line_number').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "cols = ['Source', 'Target', 'Lines', 'CR']\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df = df.iloc[:,:-2]\n",
        "df"
      ],
      "metadata": {
        "id": "Fh27w8VgXnw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB441x104K-o"
      },
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console=Console(record=True)\n",
        "\n",
        "def display_df(df):\n",
        "  \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "  console=Console()\n",
        "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
        "\n",
        "  for i, row in enumerate(df.values.tolist()):\n",
        "    table.add_row(row[0], row[1])\n",
        "\n",
        "  console.print(table)\n",
        "\n",
        "def plot_loss(index_list, loss_list):\n",
        "  results = {\n",
        "      \"epochs\": index_list,\n",
        "      \"cross entropy loss\": loss_list,\n",
        "  }\n",
        "  df = pd.DataFrame(results)\n",
        "  df.to_csv('t5-loss.csv')\n",
        "  fig = px.line(df, x =\"epochs\", y=\"cross entropy loss\",  title=\"Evaluation\")\n",
        "  fig.show()\n",
        "\n",
        "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
        "                        Column(\"Steps\", justify=\"center\"),\n",
        "                        Column(\"Cross Entropy Loss\", justify=\"center\"), \n",
        "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "class YourDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "    \n",
        "    source = self.tokenizer.batch_encode_plus([source_text], \\\n",
        "                                              max_length= self.source_len, \\\n",
        "                                              pad_to_max_length=True, \\\n",
        "                                              truncation=True, \\\n",
        "                                            #  padding='longest',\\\n",
        "                                              return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], \\\n",
        "                                              max_length= self.summ_len, \\\n",
        "                                              pad_to_max_length=True, \\\n",
        "                                              truncation=True, \\\n",
        "                                           #   padding='longest', \\\n",
        "                                              return_tensors='pt')   \n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkj6wIMt40RK"
      },
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to be called for training with the parameters passed from main function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "  loss_list = []\n",
        "  for _, data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, \\\n",
        "                    decoder_input_ids=y_ids, labels=lm_labels)  \n",
        "    total_loss = float(outputs[0].item())\n",
        "    loss_list.append(total_loss)\n",
        "    if _%10==0:\n",
        "      training_logger.add_row(str(epoch), str(_), str(total_loss))\n",
        "      console.print(training_logger)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs[0].backward()\n",
        "    optimizer.step()\n",
        "    return loss_list\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUBykK-A43DF"
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=700, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=5.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          print(\"generated token length: \\n\", len(generated_ids[0]))\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, \\\n",
        "                                    clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, \\\n",
        "                                     clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  print('predictions: \\n', predictions)\n",
        "  print('actuals: \\n', actuals)\n",
        "  return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw4RW_qO4_8T"
      },
      "source": [
        "import gc\n",
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./output/\"):\n",
        "  \n",
        "  \"\"\"\n",
        "  T5 trainer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # logging\n",
        "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3'\n",
        "  os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model = model.to(device)\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "  # Importing the raw dataset\n",
        "  dataframe = dataframe[[source_text,target_text]]\n",
        "  display_df(dataframe.head(2))\n",
        "\n",
        "  \n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  train_size = 0.8\n",
        "  train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "  console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "  optimizer = torch.optim.Adam(params = model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  loss_result = []\n",
        "  epoch_list = []\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      loss = train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "      loss_result.extend(loss)\n",
        "      epoch_list.append(epoch)\n",
        "\n",
        "  plot_loss(epoch_list, loss_result)\n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join(output_dir, \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "\n",
        "\n",
        "  # evaluating test dataset\n",
        "  console.log(f\"[Initiating Validation]...\\n\")\n",
        "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "  \n",
        "  console.log(f\"[Validation Completed.]\\n\")\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "32N828qwdXf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fcecd9-9b82-4c64-c153-48ddf1016269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxCpQwD8PDIs"
      },
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"t5-small\",             # model_type: t5-large\n",
        "    \"TRAIN_BATCH_SIZE\": 8,          # training batch size: \n",
        "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 32,              # number of training epochs:20 seems optimal based on experiments\n",
        "    \"VAL_EPOCHS\":3,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":0.001,          # learning rate: can be 1e-4\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 1000,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 1000,   # max length of target text\n",
        "    \"SEED\": random.randint(1000)    # randomized seeds to shuffle test set\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To solve CUDA out of memory error\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0, 1, 2, 3'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "metadata": {
        "id": "BfUkAFl535nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qijZoYeI55fM"
      },
      "source": [
        "T5Trainer(dataframe=df, source_text=\"Source\", target_text=\"Target\", model_params=model_params, output_dir=\"outputs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_params = {\n",
        "      'batch_size': 8,\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "class TestDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, source_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.source_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    print('source_text: ', source_text)\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], \\\n",
        "                                              max_length=self.source_len, \\\n",
        "                                              pad_to_max_length=True, \\\n",
        "                                              truncation=True, \\\n",
        "                                              padding=\"max_length\",\\\n",
        "                                              return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "    }"
      ],
      "metadata": {
        "id": "2b6yu6USc6xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_contract = 'ethertest.sol'"
      ],
      "metadata": {
        "id": "aPjKhHelWdqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc\n",
        "#let's get the trained model and never-seen test contracts \n",
        "def initialize():\n",
        "  gc = gspread.authorize(creds)\n",
        "  trained_model = T5ForConditionalGeneration.from_pretrained(\"outputs/model_files\")\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "  col = ['Test']\n",
        "  f = open(f'/content/drive/MyDrive/experiments/baseline benchmark/{test_contract}', \"r\")\n",
        "  file = f.read()\n",
        "  test_df = pd.DataFrame([file], columns=col)\n",
        "  #Convert to a DataFrame \n",
        "  return test_df, trained_model, tokenizer\n",
        "\n",
        "#test how the model performs on never-seen test contracts\n",
        "def generate(df, model, tokenizer):\n",
        "   model.eval()\n",
        "   test_val = TestDataSetClass(df, tokenizer, source_len=512, source_text=\"Test\")\n",
        "   test_loader = DataLoader(test_val, **test_params)\n",
        "   predictions = []\n",
        "   with torch.no_grad():\n",
        "      for _, data in enumerate(test_loader, 0):\n",
        "          ids = data['source_ids']\n",
        "          mask = data['source_mask'] #.to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, \\\n",
        "                                    clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          print(\"preds: \", preds)\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "          predictions.extend(preds)\n",
        "         # print('predictions: \\n', predictions)\n",
        "   return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "978OkUbHWG5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2, trained_model, tokenizer = initialize()"
      ],
      "metadata": {
        "id": "2zX3sk0b2JPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a2c1b3-89d3-43df-ee0c-4a416f49998d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning:\n",
            "\n",
            "This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = generate(df2, trained_model, tokenizer)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "XFwuoLwH2WFT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a4b5983e-f40b-4cf0-b9c6-81a43776dcc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_text:  1 / SPDX-License-Identifier: MIT 2 pragma solidity >=0.4.24 <0.6.0; 3 4 contract ethertest{ 5 function pay(address[] recipients, 6 uint256[] amounts) { 7 require(recipients.length==amounts.length); 8 for (uint i = 0; i < recipients.length; i++) { 9 recipients[i].send(amounts[i]); 10 } 11 } 12 }\n",
            "preds:  ['MIT 2 pragma solidity >= 0,24 0.6.0; 3 4 contract ethertest(amounts[i].send(amounts[i]); 9+ require(amounts[i]);']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Completed \u001b[1;36m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MIT 2 pragma solidity >= 0,24 0.6.0; 3 4 contract ethertest(amounts[i].send(amounts[i]); 9+ require(amounts[i]);']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_line_and_invariant(prediction):  \n",
        "  line_df = []\n",
        "  for item in prediction:   \n",
        "    for line in item.split(';'):\n",
        "      line_df.append(line)\n",
        "  invariants = []\n",
        "  for i in range(len(line_df)-1): \n",
        "      letter_index = 0\n",
        "      letter = line_df[i][0]\n",
        "      line_number = ''\n",
        "      invariant = ''\n",
        "      while letter != '+':  \n",
        "        line_number += letter \n",
        "        letter_index += 1  \n",
        "        if letter_index > (len(line_df[i]) - 1):\n",
        "          break        \n",
        "        letter = line_df[i][letter_index]        \n",
        "        if letter_index >= 5:\n",
        "          line_number = ''        \n",
        "          break \n",
        "      if letter_index >= 5:\n",
        "        line_number = '' \n",
        "        invariant = line_df[i][0:len(line_df[i])] \n",
        "      else: \n",
        "        invariant = line_df[i][letter_index+1:len(line_df[i])]\n",
        "      invariants.append([line_number.replace(\" \", \"\"), invariant])\n",
        "  return invariants\n",
        "\n",
        "def insert_and_annotate(test_contract, invariants): \n",
        "  new_template = open(f'/content/drive/MyDrive/experiments/evaluation/T5_annotated_contracts/{test_contract}(annotated)', \"w\")\n",
        "  for line in df2['Test'][0].split('\\n'): \n",
        "      if line != '':   \n",
        "        for item in invariants: \n",
        "          if item[0] == line[:3].replace(\" \", \"\") and item[0] != '':            \n",
        "            comment = '//[SCInvarinfer] inserting inferred invarint below: \\n'\n",
        "            line = '\\n'+ comment + line + '\\n' + '\\t' + item[1] + ';'\n",
        "            new_template.write(line + '\\n')    \n",
        "      new_template.write(line + '\\n')\n",
        "      print(line)\n",
        "  new_template.close()"
      ],
      "metadata": {
        "id": "89W6bBT0L4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invariants = get_line_and_invariant(out)\n",
        "\n",
        "def save_invariants(test_contract, invariants):\n",
        "  new_template = open(f'/content/drive/MyDrive/experiments/evaluation/T5_inferred_invariants/{test_contract}(invariants)', \"w\")\n",
        "  for line in invariants:       \n",
        "      new_template.write(str(line) + '\\n')\n",
        "      print(line)\n",
        "  new_template.close()\n"
      ],
      "metadata": {
        "id": "VGxRCIVFBzEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_invariants(test_contract, invariants)"
      ],
      "metadata": {
        "id": "1XMP0VP4lN7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insert_and_annotate(test_contract, invariants)"
      ],
      "metadata": {
        "id": "YBv-r_8Wczoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for downloading purpose \n",
        "!zip -r /content/T5-model.zip /content/outputs/model_files"
      ],
      "metadata": {
        "id": "FmpWiJGphwqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59848fdd-4afd-4722-b1bf-811e4834b4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/outputs/model_files/ (stored 0%)\n",
            "updating: content/outputs/model_files/pytorch_model.bin\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/T5-model.zip\")"
      ],
      "metadata": {
        "id": "0oBGStG04VKy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4cbc83b8-79d9-4fec-8e2d-c6459faefc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e8b38eae-e570-4209-8cbe-4b6207fc1ebd\", \"T5-model.zip\", 222828420)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}